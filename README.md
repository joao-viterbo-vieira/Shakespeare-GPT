
# Decoder-only Transformer Implementation

This repository contains the implementation of a Decoder-only Transformer developed jointly during Andrej Karpathy’s online class on YouTube. It is based on the seminal “Attention Is All You Need” paper by Vaswani et al. (2017).

## About

This project is part of a class focused on understanding and implementing the core concepts of the Transformer architecture, specifically the decoder-only variant that underlies modern large language models. The goal is to train a Transformer-based model to replicate the style and structure of Shakespearean writing using a dataset provided in input.txt, which contains over 1.1 million characters from Shakespeare’s works. By learning to predict the next character in a sequence, the model captures the patterns, vocabulary, and poetic form characteristic of Shakespeare’s language. Through this process, the project demonstrates how a decoder-only Transformer can be trained from scratch to generate text, providing hands-on insight into the mechanics of contemporary language models.

## Reference Papers

The repository includes the following important research papers:

- **1706.03762v7.pdf** - "Attention Is All You Need" (Vaswani et al., 2017)
- **1607.06450v1.pdf** - Layer Normalization research paper

- **2005.14165v4.pdf** - Language modeling and related concepts

- **1512.03385v1.pdf** - Deep Residual Learning paper

- **srivastava14a.pdf** - Dropout: A Simple Way to Prevent Neural Networks from Overfitting

## Key Concepts Covered

- Self-attention mechanisms
- Multi-head attention
- Positional encoding
- Layer normalization
- Feed-forward networks
- Decoder-only architecture (GPT-style)

## Acknowledgments

- Karpathy class explanation and implementation: https://youtu.be/kCc8FmEb1nY?si=hrrc363p1WMasa16

---

*This implementation serves as an educational exploration of the transformer architecture that revolutionized natural language processing and became the foundation for modern AI systems.*

